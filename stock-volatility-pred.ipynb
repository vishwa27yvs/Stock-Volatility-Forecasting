{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 564,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import glob\n",
    "import torch\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 565,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"/Users/vishwa/Desktop/MAEC-master/MAEC_Dataset\"\n",
    "file_list = os.listdir(file_path)\n",
    "file_list.remove('.DS_Store')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 566,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selecting earning calls from the year 2018\n",
    "subset=[]\n",
    "companies=[]\n",
    "\n",
    "start = 2018\n",
    "end = 2018\n",
    "\n",
    "# Only considering a subset of data\n",
    "for file in file_list:\n",
    "    \n",
    "    year= int(file[:4])\n",
    "    \n",
    "    if year >= start and year < (end+1):\n",
    "        subset.append(file)\n",
    "        \n",
    "    company = file[9:]\n",
    "    if company not in companies:\n",
    "        companies.append(company)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 567,
   "metadata": {},
   "outputs": [],
   "source": [
    "earning_calls = sorted(subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Mean pitch', 'Standard deviation', 'Minimum pitch', 'Maximum pitch',\n",
      "       'Mean intensity', 'Minimum intensity', 'Maximum intensity',\n",
      "       'Number of pulses', 'Number of periods', 'Mean period',\n",
      "       'Standard deviation of period', 'Fraction of unvoiced',\n",
      "       'Number of voice breaks', 'Degree of voice breaks', 'Jitter local',\n",
      "       'Jitter local absolute', 'Jitter rap', 'Jitter ppq5', 'Jitter ddp',\n",
      "       'Shimmer local', 'Shimmer local dB', 'Shimmer apq3', 'Shimmer apq5',\n",
      "       'Shimmer apq11', 'Shimmer dda', 'Mean autocorrelation', 'Mean NHR',\n",
      "       'Mean HNR', 'Audio Length'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "29"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to check features used \n",
    "print(df.columns)\n",
    "len(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "320"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(earning_calls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max number of sentences in a call: 444\n",
      "Avg number of sentences in a call: 174.29375\n",
      "Min Date: 20180104\n",
      "Max Date: 20180621\n"
     ]
    }
   ],
   "source": [
    "cnt_ineq=0\n",
    "file_ineq=0\n",
    "max_len=0\n",
    "min_date=30000000\n",
    "max_date=0\n",
    "tot_len=0\n",
    "\n",
    "for i in range(len(earning_calls)):\n",
    "#for i in range(5): \n",
    "    file = earning_calls[i]\n",
    "        \n",
    "    text_file_path = r'/Users/vishwa/Desktop/MAEC-master/MAEC_Dataset/'+ file +'/text.txt'\n",
    "    f = open(text_file_path, \"r\")\n",
    "    # to display file content\n",
    "    content =f.read()\n",
    "    lines = content.split(\"\\n\") # split at new line character\n",
    "    \n",
    "    if lines[-1]=='':\n",
    "        lines = lines[:-1] # last line blank removal\n",
    "    \n",
    "    aud_file_path = r'/Users/vishwa/Desktop/MAEC-master/MAEC_Dataset/'+ file +'/features.csv'\n",
    "    df = pd.read_csv(aud_file_path)\n",
    "    \n",
    "    # each sentence is mapped with the corresponding audio features\n",
    "    # number of sentences in an earning call\n",
    "    num_of_sent= len(lines)\n",
    "    \n",
    "    if len(lines)!=df.shape[0]:\n",
    "        print(\"error\")\n",
    "        file_no=i\n",
    "        cnt_ineq += 1\n",
    "        \n",
    "    if num_of_sent > max_len:\n",
    "        max_len=num_of_sent\n",
    "        \n",
    "    tot_len+= len(lines)\n",
    "        \n",
    "    val = int(file[:8])\n",
    "        \n",
    "    if val < min_date:\n",
    "        min_date = val\n",
    "    \n",
    "    if val > max_date:\n",
    "        max_date = val\n",
    "        \n",
    "print(\"Max number of sentences in a call: {}\".format(max_len))\n",
    "print(\"Avg number of sentences in a call: {}\".format(tot_len/len(earning_calls)))\n",
    "print(\"Min Date: {}\".format(min_date))\n",
    "print(\"Max Date: {}\".format(max_date))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping volatility data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from io import StringIO\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import requests\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our data spans these dates\n",
    "# 1 st Jan 2018 to 31 st Dec 2018 \n",
    "# yahoo finance historical data\n",
    "\n",
    "def get_yahoofinance_hist(company_idx):\n",
    "    session = requests.Session()\n",
    "    # period1 and period2 for max and min date = company value can be formatted\n",
    "    download_link = 'https://query1.finance.yahoo.com/v7/finance/download/{company}?period1=1514764800&period2=1546214400&interval=1d&events=history&includeAdjustedClose=true'\n",
    "#https://query1.finance.yahoo.com/v7/finance/download/GPN?period1=1514764800&period2=1546214400&interval=1d&events=history&includeAdjustedClose=true\n",
    "    try:\n",
    "        url = download_link.format(company=company_idx)\n",
    "        response = session.get(url)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        if response.status_code != 404:\n",
    "            df = pd.read_csv(StringIO(response.text), parse_dates=['Date'])\n",
    "    \n",
    "            # filtering as we only need close price\n",
    "            df_fil = df[['Date','Close']]\n",
    "            \n",
    "            return df_fil\n",
    "        else:\n",
    "            #print(\"exc1\")\n",
    "            return -1\n",
    "    \n",
    "    except:\n",
    "        #print(\"exc2\")\n",
    "        #error_info.append(company_idx)\n",
    "        return -1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_yahoofinance_hist('K')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Close</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-01-02</td>\n",
       "      <td>67.970001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-01-03</td>\n",
       "      <td>67.650002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-01-04</td>\n",
       "      <td>68.730003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-01-05</td>\n",
       "      <td>68.940002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-01-08</td>\n",
       "      <td>69.169998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245</th>\n",
       "      <td>2018-12-21</td>\n",
       "      <td>57.639999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>2018-12-24</td>\n",
       "      <td>55.820000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>2018-12-26</td>\n",
       "      <td>57.150002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>2018-12-27</td>\n",
       "      <td>57.139999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>2018-12-28</td>\n",
       "      <td>57.250000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>250 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Date      Close\n",
       "0   2018-01-02  67.970001\n",
       "1   2018-01-03  67.650002\n",
       "2   2018-01-04  68.730003\n",
       "3   2018-01-05  68.940002\n",
       "4   2018-01-08  69.169998\n",
       "..         ...        ...\n",
       "245 2018-12-21  57.639999\n",
       "246 2018-12-24  55.820000\n",
       "247 2018-12-26  57.150002\n",
       "248 2018-12-27  57.139999\n",
       "249 2018-12-28  57.250000\n",
       "\n",
       "[250 rows x 2 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "closePrice={}\n",
    "error_info=[]\n",
    "error_info_idx=[]\n",
    "\n",
    "# scraping data for all the companies and removing those for which data could not be scraped\n",
    "for i in range(len(earning_calls)):\n",
    "    company_idx = earning_calls[i][9:]\n",
    "    \n",
    "    if company_idx not in closePrice.keys():\n",
    "        df = get_yahoofinance_hist(company_idx)\n",
    "        if type(df) == int: # in case when the url was not accessible\n",
    "            error_info.append(earning_calls[i])\n",
    "            error_info_idx.append(i)\n",
    "        else:    \n",
    "            closePrice[company_idx] = df\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "320"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(earning_calls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "238"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dictionary of closing price for various companies\n",
    "len(closePrice.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "fil_calls = earning_calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing all calls for which data was unavailable\n",
    "for i in sorted(error_info_idx, reverse=True):\n",
    "    #print(i)\n",
    "    del fil_calls[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "298"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# filtered calls\n",
    "len(fil_calls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = '20180510_FTK'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp = x[9:]\n",
    "df = closePrice[comp]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "datetime_object = datetime.strptime('2018-05-10', '%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime(2018, 5, 10, 0, 0)"
      ]
     },
     "execution_count": 284,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datetime_object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices=[]\n",
    "tou = 3 # timesteps\n",
    "grdVals =np.zeros((len(fil_calls),2*tou+2))\n",
    "idx =0\n",
    "# there will be 8 timesteps as we need p(i-1) and p(i) to calculate the return price\n",
    "\n",
    "#for i in range(1):\n",
    "for call in fil_calls:\n",
    "    #call = fil_calls[i]\n",
    "    comp = call[9:]\n",
    "    df =  closePrice[comp]\n",
    "    \n",
    "    date = call[:4]+'-'+call[4:6]+'-'+ call[6:8]\n",
    "    \n",
    "    # idx in the dataframe\n",
    "    dt =-1\n",
    "    \n",
    "    for i in range(df.shape[0]):\n",
    "        \n",
    "        if str(df.iloc[i]['Date'])[:10]==date:\n",
    "            dt =i\n",
    "            indices.append(dt)\n",
    "    \n",
    "    if dt == -1:\n",
    "        print(\"error\")\n",
    "    \n",
    "    # taking 3 values before and 3 values after - tou = 3 for window\n",
    "    vals=[]\n",
    "    \n",
    "    if (dt-tou-1)<0:\n",
    "        # use the same value as close price day\n",
    "        pre = [df.iloc[dt]['Close']]*(tou+1)\n",
    "        vals = vals + pre\n",
    "    else:\n",
    "        # prev 4 days\n",
    "        vals = vals + list(df.iloc[dt-tou-1:dt]['Close'])\n",
    "    \n",
    "    vals = vals+ [df.iloc[dt]['Close']]\n",
    "    \n",
    "    if (dt+tou)>(df.shape[0]):\n",
    "        # use the same value as close price day\n",
    "        post = [df.iloc[dt]['Close']]*(tou)\n",
    "        vals = vals + post\n",
    "    else:\n",
    "        vals= vals + list(df.iloc[dt+1:dt+tou+1]['Close'])\n",
    "    \n",
    "    #print(idx)\n",
    "    grdVals[idx]= vals\n",
    "    idx+=1\n",
    "    \n",
    "\n",
    "grdVals = np.array(grdVals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[43.799999,\n",
       " 43.700001,\n",
       " 43.349998,\n",
       " 45.0,\n",
       " 40.950001,\n",
       " 39.900002,\n",
       " 40.099998,\n",
       " 41.049999]"
      ]
     },
     "execution_count": 301,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(298, 8)"
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grdVals.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'20180123_HAFC'"
      ]
     },
     "execution_count": 303,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fil_calls[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([32.049999, 31.700001, 32.200001, 32.200001, 32.099998, 31.75    ,\n",
       "       31.950001, 31.6     ])"
      ]
     },
     "execution_count": 304,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grdVals[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "grdVals_df= pd.DataFrame(grdVals)\n",
    "grdVals_df.to_csv(r'/Users/vishwa/Desktop/grdVals.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approach - Getting Direct sentence embeddings from SentenceBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# to get the sentence_model\n",
    "sentence_model = SentenceTransformer('bert-base-nli-mean-tokens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/vishwa/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [],
   "source": [
    "document = [\"I ate dinner.\", \n",
    "       \"Bedford is an existing site that we have.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sent_embed(embed,max_len):\n",
    "    \n",
    "    # as we dont want to lose context, we will add blank sentence as a prembedding\n",
    "    #if len(embed)>120:\n",
    "        # take first 120 lines\n",
    "        #lines=lines[:120]\n",
    "    #else:\n",
    "        # zero vector\n",
    "    dim_size = 768\n",
    "    zer_vec = np.zeros((max_len-len(embed),dim_size))\n",
    "    embed = np.concatenate((zer_vec,embed),axis=0)\n",
    "        \n",
    "    return embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_sent = []\n",
    "for sentence in document:\n",
    "    token_sent.append(word_tokenize(sentence.lower()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = sentence_model.encode(document)\n",
    "import pickle\n",
    "\n",
    "#Store sentences & embeddings on disc\n",
    "with open('/Users/vishwa/Desktop/embeds/embeddings.pkl', \"wb\") as fOut:\n",
    "    pickle.dump({'sentences': document, 'embeddings': embeddings}, fOut, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "#Load sentences & embeddings from disc\n",
    "with open('/Users/vishwa/Desktop/embeds/embeddings.pkl', \"rb\") as fIn:\n",
    "    stored_data = pickle.load(fIn)\n",
    "    stored_sentences = stored_data['sentences']\n",
    "    stored_embeddings = stored_data['embeddings']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 768)"
      ]
     },
     "execution_count": 420,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stored_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_embeddings = sentence_model.encode(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sentence_embeddings[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_embeddings=pad_sent_embed(sentence_embeddings)\n",
    "sentence_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 554,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max number of sentences in a call: 444\n",
      "Avg number of sentences in a call: 175.29530201342283\n",
      "Iterations complete 200\n"
     ]
    }
   ],
   "source": [
    "cnt_ineq=0\n",
    "file_ineq=0\n",
    "max_len=0\n",
    "min_date=30000000\n",
    "max_date=0\n",
    "tot_len=0\n",
    "\n",
    "\n",
    "\n",
    "for i in range(len(fil_calls)):\n",
    "#for i in range(5): \n",
    "    file =fil_calls[i]\n",
    "          \n",
    "    text_file_path = r'/Users/vishwa/Desktop/MAEC-master/MAEC_Dataset/'+ file +'/text.txt'\n",
    "    f = open(text_file_path, \"r\")\n",
    "    # to display file content\n",
    "    content =f.read()\n",
    "    lines = content.split(\"\\n\") # split at new line character\n",
    "    \n",
    "    if lines[-1]=='':\n",
    "        lines = lines[:-1] # last line blank removal\n",
    "    \n",
    "    aud_file_path = r'/Users/vishwa/Desktop/MAEC-master/MAEC_Dataset/'+ file +'/features.csv'\n",
    "    df = pd.read_csv(aud_file_path)\n",
    "    \n",
    "    # each sentence is mapped with the corresponding audio features\n",
    "    # number of sentences in an earning call\n",
    "    num_of_sent= len(lines)\n",
    "    \n",
    "    if len(lines)!=df.shape[0]:\n",
    "        print(\"error\")\n",
    "        file_no=i\n",
    "        cnt_ineq += 1\n",
    "        \n",
    "    if num_of_sent > max_len:\n",
    "        max_len=num_of_sent\n",
    "        \n",
    "    tot_len+= len(lines)\n",
    "        \n",
    "    val = int(file[:8])\n",
    "\n",
    "print(\"Max number of sentences in a call: {}\".format(max_len))\n",
    "print(\"Avg number of sentences in a call: {}\".format(tot_len/len(fil_calls)))\n",
    "\n",
    "text_data = np.zeros((len(fil_calls),max_len,768))\n",
    "\n",
    "for i in range(len(fil_calls)):\n",
    "#for i in range(201,298):\n",
    "#for i in range(200,201):\n",
    "    file =fil_calls[i]\n",
    "    \n",
    "    text_file_path = r'/Users/vishwa/Desktop/MAEC-master/MAEC_Dataset/'+ file +'/text.txt'\n",
    "    f = open(text_file_path, \"r\")\n",
    "    # to display file content\n",
    "    content =f.read()\n",
    "    lines = content.split(\"\\n\") # split at new line character\n",
    "    \n",
    "    if lines[-1]=='':\n",
    "        lines = lines[:-1] # last line blank removal\n",
    "        \n",
    "    sentence_embeddings = sentence_model.encode(lines)\n",
    "    sentence_embeddings_padded = pad_sent_embed(sentence_embeddings,max_len)\n",
    "    \n",
    "    text_data[i] = np.array(sentence_embeddings_padded)\n",
    "    \n",
    "    with open('/Users/vishwa/Desktop/embeds/'+ file +'.pkl', \"wb\") as fOut:\n",
    "        pickle.dump({'embeddings': sentence_embeddings_padded}, fOut, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "    if i%10==0:\n",
    "        print(\"Iterations complete {}\".format(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('/Users/vishwa/Desktop/embeds/20180122_SFBS.pkl', \"rb\") as fIn:\n",
    "    stored_data = pickle.load(fIn)\n",
    "    stored_embeddings = stored_data['embeddings']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 555,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"/Users/vishwa/Desktop/embeds\"\n",
    "file_list = os.listdir(file_path)\n",
    "file_list.remove('.DS_Store')\n",
    "file_list = sorted(file_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "textData = np.zeros((len(fil_calls),max_len,768))\n",
    "\n",
    "for i in range(len(fil_calls)):\n",
    "    \n",
    "    with open('/Users/vishwa/Desktop/embeds/'+fil_calls[i]+'.pkl', \"rb\") as fIn:\n",
    "        stored_data = pickle.load(fIn)\n",
    "        stored_embeddings = stored_data['embeddings']\n",
    "    \n",
    "    textData[i] = np.array(stored_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(298, 444, 768)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textData.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pad_sequences\n",
    "!pip install tensorflow\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_list=[]\n",
    "for i in range(len(fil_calls)):\n",
    "    comp = fil_calls[i][9:]\n",
    "    if comp not in comp_list:\n",
    "        comp_list.append(comp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "238"
      ]
     },
     "execution_count": 387,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(comp_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Populating Audio Feature Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max number of audio utterances in a call: 444\n",
      "Avg number of sentences in a call: 175.29530201342283\n",
      "Min Date: 20180104\n",
      "Max Date: 20180605\n"
     ]
    }
   ],
   "source": [
    "cnt_ineq=0\n",
    "file_ineq=0\n",
    "max_seg=0\n",
    "min_date=30000000\n",
    "max_date=0\n",
    "tot_seg=0\n",
    "\n",
    "for i in range(len(fil_calls)):\n",
    "#for i in range(5): \n",
    "    file =fil_calls[i]\n",
    "            \n",
    "    text_file_path = r'/Users/vishwa/Desktop/MAEC-master/MAEC_Dataset/'+ file +'/text.txt'\n",
    "    f = open(text_file_path, \"r\")\n",
    "    # to display file content\n",
    "    content =f.read()\n",
    "    lines = content.split(\"\\n\") # split at new line character\n",
    "    \n",
    "    if lines[-1]=='':\n",
    "        lines = lines[:-1] # last line blank removal\n",
    "    \n",
    "    aud_file_path = r'/Users/vishwa/Desktop/MAEC-master/MAEC_Dataset/'+ file +'/features.csv'\n",
    "    df = pd.read_csv(aud_file_path)\n",
    "    \n",
    "    # each sentence is mapped with the corresponding audio features\n",
    "    # number of audio utterance segments in an earning call\n",
    "    num_of_seg= df.shape[0]\n",
    "    \n",
    "    if len(lines)!=df.shape[0]:\n",
    "        print(\"error\")\n",
    "        file_no=i\n",
    "        cnt_ineq += 1\n",
    "        \n",
    "    if num_of_seg > max_seg:\n",
    "        max_seg=num_of_seg\n",
    "        \n",
    "    tot_seg+= num_of_seg\n",
    "        \n",
    "    val = int(file[:8])\n",
    "        \n",
    "    if val < min_date:\n",
    "        min_date = val\n",
    "    \n",
    "    if val > max_date:\n",
    "        max_date = val\n",
    "        \n",
    "print(\"Max number of audio utterances in a call: {}\".format(max_seg))\n",
    "print(\"Avg number of sentences in a call: {}\".format(tot_seg/len(fil_calls)))\n",
    "print(\"Min Date: {}\".format(min_date))\n",
    "print(\"Max Date: {}\".format(max_date))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need to pad the sequences\n",
    "# and also truncate the sentences as this can cause sparsity issues\n",
    "\n",
    "num_calls = len(fil_calls)\n",
    "num_features = 29\n",
    "num_segments = max_len\n",
    "\n",
    "audioData = np.zeros((num_calls,num_segments,num_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(298, 444, 29)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(audioData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to pad audio segment data or truncate if necessary\n",
    "def pad_aud_seg(audio_features):\n",
    "    # as we dont want to lose context, we will add blank sentence as a prembedding\n",
    "    # as the last hidden state captures context\n",
    "    num_features = audio_features.shape[1] #29\n",
    "    \n",
    "    # 120 audio segments ~ avg 114\n",
    "    # to avoid high dimensionality\n",
    "    max_seg_len = max_len\n",
    "    seg = audio_features.shape[0]\n",
    "    if seg >= max_seg_len:\n",
    "        # take first 120 segments - truncate\n",
    "        padded_audio = np.array(audio_features[:max_seg_len,:])\n",
    "    else:\n",
    "        zero_padding = [[0]*num_features]*(max_seg_len-seg)\n",
    "        padded_audio = np.concatenate((zero_padding,audio_features),axis=0)\n",
    "        \n",
    "    return padded_audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files completed : 0\n",
      "Files completed : 100\n",
      "Files completed : 200\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "idx = 0\n",
    "\n",
    "for i in range(len(fil_calls)):\n",
    "#for i in range(5): \n",
    "    file =fil_calls[i]\n",
    "        \n",
    "    text_file_path = r'/Users/vishwa/Desktop/MAEC-master/MAEC_Dataset/'+ file +'/text.txt'\n",
    "    f = open(text_file_path, \"r\")\n",
    "    # to display file content\n",
    "    content =f.read()\n",
    "    lines = content.split(\"\\n\") # split at new line character\n",
    "    \n",
    "    if lines[-1]=='':\n",
    "        lines = lines[:-1] # last line blank removal\n",
    "    \n",
    "    aud_file_path = r'/Users/vishwa/Desktop/MAEC-master/MAEC_Dataset/'+ file +'/features.csv'\n",
    "    df = pd.read_csv(aud_file_path)\n",
    "    \n",
    "    # each sentence is mapped with the corresponding audio features\n",
    "    # number of audio utterance segments in an earning call\n",
    "    num_of_seg= df.shape[0]\n",
    "    \n",
    "    if len(lines)!=df.shape[0]:\n",
    "        print(\"error\")\n",
    "        file_no=i\n",
    "        cnt_ineq += 1\n",
    "        \n",
    "    #print(idx)\n",
    "    #print(num_of_seg)\n",
    "    \n",
    "    # replacing undefined str type\n",
    "    df = df.replace('--undefined--', float('nan'))\n",
    "    df = df.replace('--undefined-', float('nan'))\n",
    "    df = df.replace('--undefined-- ', float('nan'))\n",
    "    # fill forward to fill these values\n",
    "    for col in df.columns:\n",
    "        #try:\n",
    "           # df[col].fillna( method ='ffill', inplace = True) \n",
    "        # to catch exception for all nan values\n",
    "        #except RuntimeWarning as e:\n",
    "            #print(\"error\")\n",
    "        df[col].fillna(0, inplace = True) \n",
    "    \n",
    "    \n",
    "    audio_feat = np.array(df)\n",
    "    # pad the audio segment for efficient batching\n",
    "    padded_values = pad_aud_seg(audio_feat)\n",
    "    \n",
    "    audioData[idx] = padded_values\n",
    "    \n",
    "    if idx%100==0:\n",
    "        print(\"Files completed : {}\".format(idx))\n",
    "    \n",
    "    idx +=1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(298, 444, 29)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audioData.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computing Past 3 day volatility and next 3 day volatility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sending a list of closing price\n",
    "def compute_volatility(close_pr,close_pr_prev):\n",
    "    \n",
    "    return_pr= [pr/pr_prev for pr,pr_prev in zip(close_pr,close_pr_prev)]\n",
    "    #return_pr = [ (pr)/(pr-1) for pr in close_pr ]\n",
    "    \n",
    "    mean_rt = np.mean(return_pr)\n",
    "    diff_rt = (return_pr-mean_rt)\n",
    "    if len(close_pr)==0:\n",
    "        print(\"error\")\n",
    "    vol = np.log(np.sqrt(sum(np.multiply(diff_rt,diff_rt))/(len(close_pr))))\n",
    "    \n",
    "    return vol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['0', '1', '2', '3', '4', '5', '6', '7'], dtype='object')"
      ]
     },
     "execution_count": 391,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# scraped and processed data\n",
    "vol_file_path = r'/Users/vishwa/Desktop/grdVals.csv'\n",
    "closing_pr = pd.read_csv(vol_file_path)\n",
    "closing_pr= closing_pr.drop(['Unnamed: 0'],axis=1)\n",
    "closing_pr.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-390-f5de3d8e28cb>:11: RuntimeWarning: divide by zero encountered in log\n",
      "  vol = np.log(np.sqrt(sum(np.multiply(diff_rt,diff_rt))/(len(close_pr))))\n"
     ]
    }
   ],
   "source": [
    "# volitility of prev 3 days\n",
    "pre_vol=[]\n",
    "\n",
    "# volitility of next 3 days -- var to be predicted\n",
    "post_vol=[]\n",
    "\n",
    "for i in range(closing_pr.shape[0]):\n",
    "    pre_curr = [closing_pr.iloc[i]['1']]+[closing_pr.iloc[i]['2']]+[closing_pr.iloc[i]['3']]\n",
    "    pre_past = [closing_pr.iloc[i]['0']]+[closing_pr.iloc[i]['1']]+[closing_pr.iloc[i]['2']]\n",
    "    \n",
    "    post_curr = [closing_pr.iloc[i]['5']]+[closing_pr.iloc[i]['6']]+[closing_pr.iloc[i]['7']]\n",
    "    post_past = [closing_pr.iloc[i]['4']]+[closing_pr.iloc[i]['5']]+[closing_pr.iloc[i]['6']]\n",
    "    \n",
    "    pre_vol.append(compute_volatility(pre_curr,pre_past))\n",
    "    post_vol.append(compute_volatility(post_curr,post_past))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr=[]\n",
    "for i in range(len(pre_vol)):\n",
    "    if pre_vol[i]!=np.float('-inf'):\n",
    "        corr.append(pre_vol[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(pre_vol)):\n",
    "    if pre_vol[i]==np.float('-inf'):\n",
    "        pre_vol[i]= np.mean(corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(post_vol)):\n",
    "    if post_vol[i]==np.float('-inf'):\n",
    "        #print(\"here\")\n",
    "        #print(i)\n",
    "        post_vol[i]= np.mean(corr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import concatenate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow\n",
    "tensorflow.random.set_seed(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1: Concatenation Layer - CNN and Past Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 538,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "feature_emb_dim = 768\n",
    "feature_aud_dim = 29 # number of audio features\n",
    "batch_size = 8\n",
    "\n",
    "# Text Encoder\n",
    "# our text already is in the form of embeddings\n",
    "input_text = keras.Input(shape=(max_len,feature_emb_dim))\n",
    "# Add 1 bidirectional LSTM\n",
    "layer1_t = layers.Bidirectional(layers.LSTM(128, return_sequences=True))(input_text)\n",
    "output_t = layers.Dense(1, activation=\"tanh\")(layer1_t)\n",
    "\n",
    "# Audio Encoder\n",
    "# our audio is in the form of features\n",
    "input_aud = keras.Input(shape=(max_len,feature_aud_dim))\n",
    "# Add 1 bidirectional LSTM\n",
    "layer1_a = layers.Bidirectional(layers.LSTM(64, return_sequences=True))(input_aud)\n",
    "output_a = layers.Dense(1, activation=\"tanh\")(layer1_a)\n",
    "\n",
    "# Fusion\n",
    "fusion = concatenate([output_t, output_a])\n",
    "fusion2 = layers.Reshape((max_len,2,1))(fusion)\n",
    "fusion3 = layers.Conv2D(filters=1,kernel_size=(4,2),padding=\"valid\",activation=\"linear\",input_shape=(max_len,2))(fusion2)\n",
    "fusion4 = layers.Reshape((fusion3.shape[1],))(fusion3)\n",
    "fusion5 = layers.Dense(128, activation=\"linear\")(fusion4)\n",
    "\n",
    "layer_txt_aud = layers.Dense(64, activation=\"linear\")(fusion5)\n",
    "\n",
    "# combined output from both text and audio\n",
    "output_txt_aud = layers.Dense(1, activation=\"linear\")(layer_txt_aud)\n",
    "\n",
    "# Input from past val\n",
    "input_past = keras.Input(shape=(1,))\n",
    "input_final = concatenate([output_txt_aud, input_past])\n",
    "\n",
    "# output volatility\n",
    "output_vol = layers.Dense(1, activation=\"linear\")(input_final)\n",
    "\n",
    "# Final Vector gives a dense layer as op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 539,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<KerasTensor: shape=(None, 441) dtype=float32 (created by layer 'reshape_51')>"
      ]
     },
     "execution_count": 539,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fusion4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 540,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tying the entire model\n",
    "model_cnn = Model(inputs=[input_text, input_aud,input_past], outputs=output_vol)\n",
    "opt = Adam(lr=1e-3, decay=1e-3 / 200)\n",
    "model_cnn.compile(loss='mean_squared_error', optimizer=opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 541,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<KerasTensor: shape=(None, 444, 29) dtype=float32 (created by layer 'input_148')>"
      ]
     },
     "execution_count": 541,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_aud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 542,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and test split\n",
    "# Since this is dependent on time series, we cannot make use of random split\n",
    "\n",
    "split= 200\n",
    "train_txt = np.array(textData[:split])\n",
    "train_aud = np.array(audioData[:split])\n",
    "train_past = np.array(pre_vol[:split])\n",
    "\n",
    "train_y = np.array(post_vol[:split])\n",
    "\n",
    "test_txt = np.array(textData[split:])\n",
    "test_aud = np.array(audioData[split:])\n",
    "test_past = np.array(pre_vol[split:])\n",
    "\n",
    "test_y = np.array(post_vol[split:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 543,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre processing the data\n",
    "# in this case we will have to preprocess each branch\n",
    "\n",
    "# scaling the audio data\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler_layer = {}\n",
    "for i in range(train_aud.shape[1]):\n",
    "    scaler_layer[i] = StandardScaler()\n",
    "    train_aud[:, i, :] = scaler_layer[i].fit_transform(train_aud[:, i, :]) \n",
    "\n",
    "for i in range(test_aud.shape[1]):\n",
    "    test_aud[:, i, :] = scaler_layer[i].transform(test_aud[:, i, :]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 544,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 444, 768)"
      ]
     },
     "execution_count": 544,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_txt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 546,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training model...\n",
      "Epoch 1/25\n",
      "25/25 [==============================] - 18s 577ms/step - loss: 3.1173\n",
      "Epoch 2/25\n",
      "25/25 [==============================] - 13s 505ms/step - loss: 1.2626\n",
      "Epoch 3/25\n",
      "25/25 [==============================] - 13s 510ms/step - loss: 1.1082\n",
      "Epoch 4/25\n",
      "25/25 [==============================] - 13s 505ms/step - loss: 1.3123\n",
      "Epoch 5/25\n",
      "25/25 [==============================] - 13s 511ms/step - loss: 0.9187\n",
      "Epoch 6/25\n",
      "25/25 [==============================] - 16s 630ms/step - loss: 0.7866\n",
      "Epoch 7/25\n",
      "25/25 [==============================] - 16s 623ms/step - loss: 0.9228\n",
      "Epoch 8/25\n",
      "25/25 [==============================] - 16s 618ms/step - loss: 0.6519\n",
      "Epoch 9/25\n",
      "25/25 [==============================] - 16s 640ms/step - loss: 0.7667\n",
      "Epoch 10/25\n",
      "25/25 [==============================] - 16s 641ms/step - loss: 0.6216\n",
      "Epoch 11/25\n",
      "25/25 [==============================] - 16s 637ms/step - loss: 0.6078\n",
      "Epoch 12/25\n",
      "25/25 [==============================] - 16s 623ms/step - loss: 0.3764\n",
      "Epoch 13/25\n",
      "25/25 [==============================] - 16s 627ms/step - loss: 0.2721\n",
      "Epoch 14/25\n",
      "25/25 [==============================] - 16s 644ms/step - loss: 0.1449\n",
      "Epoch 15/25\n",
      "25/25 [==============================] - 16s 626ms/step - loss: 0.1089\n",
      "Epoch 16/25\n",
      "25/25 [==============================] - 16s 628ms/step - loss: 0.1110\n",
      "Epoch 17/25\n",
      "25/25 [==============================] - 16s 636ms/step - loss: 0.1037\n",
      "Epoch 18/25\n",
      "25/25 [==============================] - 15s 615ms/step - loss: 0.0787\n",
      "Epoch 19/25\n",
      "25/25 [==============================] - 14s 563ms/step - loss: 0.0573\n",
      "Epoch 20/25\n",
      "25/25 [==============================] - 15s 595ms/step - loss: 0.0444\n",
      "Epoch 21/25\n",
      "25/25 [==============================] - 15s 595ms/step - loss: 0.0387\n",
      "Epoch 22/25\n",
      "25/25 [==============================] - 14s 568ms/step - loss: 0.0199\n",
      "Epoch 23/25\n",
      "25/25 [==============================] - 14s 564ms/step - loss: 0.0178\n",
      "Epoch 24/25\n",
      "25/25 [==============================] - 14s 568ms/step - loss: 0.0172\n",
      "Epoch 25/25\n",
      "25/25 [==============================] - 14s 566ms/step - loss: 0.0196\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1e3252b20>"
      ]
     },
     "execution_count": 546,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train the model\n",
    "print(\"training model...\")\n",
    "model_cnn.fit(\n",
    "x=[train_txt, train_aud,train_past], y=train_y,\n",
    "epochs=25, batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 547,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13/13 [==============================] - 2s 101ms/step - loss: 1.9354\n",
      "test loss, test acc: 1.9353705644607544\n"
     ]
    }
   ],
   "source": [
    "results = model_cnn.evaluate([test_txt,test_aud,test_past], test_y, batch_size=8)\n",
    "print(\"test loss, test acc:\", results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to check predictions\n",
    "preds = model_cnn.predict([test_txt,test_aud,test_past])\n",
    "for i in range(len(preds)):\n",
    "    print(\"{} {}\".format(preds[i][0],test_y[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 568,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 569,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 552,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.models import model_from_json\n",
    "\n",
    "# serialize model to JSON\n",
    "model_json = model_cnn.to_json()\n",
    "with open(\"cnn3.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model_cnn.save_weights(\"cnn3.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 553,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n",
      "mean_squared_error: 1.9354\n"
     ]
    }
   ],
   "source": [
    "# load json and create model\n",
    "json_file = open(\"cnn3.json\", 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "loaded_model.load_weights(\"cnn3.h5\")\n",
    "print(\"Loaded model from disk\")\n",
    " \n",
    "# evaluate loaded model on test data\n",
    "loaded_model.compile(loss='mean_squared_error', optimizer=opt,metrics=['mean_squared_error'])\n",
    "score = loaded_model.evaluate([test_txt,test_aud,test_past],test_y, verbose=0)\n",
    "print(\"%s: %.4f\" % (loaded_model.metrics_names[1], score[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 554,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(98, 444, 768)"
      ]
     },
     "execution_count": 554,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_txt.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2: Concatenation Layer - LSTM and Past Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 555,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "feature_emb_dim = 768\n",
    "feature_aud_dim = 29\n",
    "batch_size = 8\n",
    "\n",
    "# Text Encoder\n",
    "# our text already is in the form of embeddings\n",
    "input_text = keras.Input(shape=(max_len,feature_emb_dim))\n",
    "# Add 1 bidirectional LSTM\n",
    "layer1_t = layers.Bidirectional(layers.LSTM(128, return_sequences=True))(input_text)\n",
    "output_t = layers.Dense(1, activation=\"tanh\")(layer1_t)\n",
    "\n",
    "# Audio Encoder\n",
    "# our text already is in the form of embeddings\n",
    "input_aud = keras.Input(shape=(max_len,feature_aud_dim))\n",
    "# Add 1 bidirectional LSTM\n",
    "layer1_a = layers.Bidirectional(layers.LSTM(64, return_sequences=True))(input_aud)\n",
    "output_a = layers.Dense(1, activation=\"tanh\")(layer1_a)\n",
    "\n",
    "# Fusion\n",
    "fusion = concatenate([output_t, output_a])\n",
    "fusion2 = layers.LSTM(1, return_sequences=True)(fusion)\n",
    "fusion3 = layers.Reshape((fusion2.shape[1],))(fusion2)\n",
    "fusion4 = layers.Dense(1, activation=\"linear\")(fusion3)\n",
    "\n",
    "layer_txt_aud = layers.Dense(64, activation=\"linear\")(fusion4)\n",
    "output_txt_aud = layers.Dense(1, activation=\"linear\")(layer_txt_aud)\n",
    "\n",
    "# Input from past val\n",
    "input_past = keras.Input(shape=(1,))\n",
    "input_final = concatenate([output_txt_aud, input_past])\n",
    "\n",
    "# output volatility\n",
    "output_vol = layers.Dense(1, activation=\"linear\")(input_final)\n",
    "\n",
    "# Final Vector gives a dense layer as op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 556,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<KerasTensor: shape=(None, 2) dtype=float32 (created by layer 'concatenate_111')>"
      ]
     },
     "execution_count": 556,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 557,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input to the model includes text input, audio input and the past volatility\n",
    "\n",
    "# Tying the entire model\n",
    "model_lstm = Model(inputs=[input_text, input_aud,input_past], outputs=output_vol)\n",
    "opt = Adam(lr=1e-3, decay=1e-3 / 200)\n",
    "model_lstm.compile(loss='mean_squared_error', optimizer=opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 558,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and test split\n",
    "\n",
    "split= 200\n",
    "train_txt = np.array(textData[:split])\n",
    "train_aud = np.array(audioData[:split])\n",
    "train_past = np.array(pre_vol[:split])\n",
    "\n",
    "train_y = np.array(post_vol[:split])\n",
    "\n",
    "test_txt = np.array(textData[split:])\n",
    "test_aud = np.array(audioData[split:])\n",
    "test_past = np.array(pre_vol[split:])\n",
    "\n",
    "test_y = np.array(post_vol[split:])\n",
    "\n",
    "# pre processing the data\n",
    "# in this case we will have to preprocess each branch\n",
    "\n",
    "# scaling the audio data\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler_layer = {}\n",
    "for i in range(train_aud.shape[1]):\n",
    "    scaler_layer[i] = StandardScaler()\n",
    "    train_aud[:, i, :] = scaler_layer[i].fit_transform(train_aud[:, i, :]) \n",
    "\n",
    "for i in range(test_aud.shape[1]):\n",
    "    test_aud[:, i, :] = scaler_layer[i].transform(test_aud[:, i, :]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 559,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(98, 444, 29)"
      ]
     },
     "execution_count": 559,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_aud.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 560,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training model...\n",
      "Epoch 1/25\n",
      "25/25 [==============================] - 22s 668ms/step - loss: 4.4434\n",
      "Epoch 2/25\n",
      "25/25 [==============================] - 16s 635ms/step - loss: 1.4576\n",
      "Epoch 3/25\n",
      "25/25 [==============================] - 16s 646ms/step - loss: 0.9263\n",
      "Epoch 4/25\n",
      "25/25 [==============================] - 16s 646ms/step - loss: 0.9313\n",
      "Epoch 5/25\n",
      "25/25 [==============================] - 19s 749ms/step - loss: 0.8013\n",
      "Epoch 6/25\n",
      "25/25 [==============================] - 18s 706ms/step - loss: 0.7130\n",
      "Epoch 7/25\n",
      "25/25 [==============================] - 16s 655ms/step - loss: 0.8059\n",
      "Epoch 8/25\n",
      "25/25 [==============================] - 16s 636ms/step - loss: 0.8150\n",
      "Epoch 9/25\n",
      "25/25 [==============================] - 17s 694ms/step - loss: 0.6955\n",
      "Epoch 10/25\n",
      "25/25 [==============================] - 17s 673ms/step - loss: 0.6840\n",
      "Epoch 11/25\n",
      "25/25 [==============================] - 17s 664ms/step - loss: 0.7217\n",
      "Epoch 12/25\n",
      "25/25 [==============================] - 17s 664ms/step - loss: 0.7870\n",
      "Epoch 13/25\n",
      "25/25 [==============================] - 16s 622ms/step - loss: 0.6301\n",
      "Epoch 14/25\n",
      "25/25 [==============================] - 18s 737ms/step - loss: 0.5926\n",
      "Epoch 15/25\n",
      "25/25 [==============================] - 17s 664ms/step - loss: 0.6152\n",
      "Epoch 16/25\n",
      "25/25 [==============================] - 17s 662ms/step - loss: 0.3945\n",
      "Epoch 17/25\n",
      "25/25 [==============================] - 17s 686ms/step - loss: 0.5124\n",
      "Epoch 18/25\n",
      "25/25 [==============================] - 16s 619ms/step - loss: 0.4616\n",
      "Epoch 19/25\n",
      "25/25 [==============================] - 15s 618ms/step - loss: 0.4847\n",
      "Epoch 20/25\n",
      "25/25 [==============================] - 16s 625ms/step - loss: 0.3202\n",
      "Epoch 21/25\n",
      "25/25 [==============================] - 15s 615ms/step - loss: 0.3078\n",
      "Epoch 22/25\n",
      "25/25 [==============================] - 16s 633ms/step - loss: 0.2537\n",
      "Epoch 23/25\n",
      "25/25 [==============================] - 16s 650ms/step - loss: 0.2663\n",
      "Epoch 24/25\n",
      "25/25 [==============================] - 16s 625ms/step - loss: 0.4006\n",
      "Epoch 25/25\n",
      "25/25 [==============================] - 17s 669ms/step - loss: 0.3268\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1cdd38e20>"
      ]
     },
     "execution_count": 560,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train the model\n",
    "print(\"training model...\")\n",
    "cnt += 1\n",
    "model_lstm.fit(\n",
    "x=[train_txt, train_aud,train_past], y=train_y,\n",
    "#validation_data=([test_txt,test_aud,test_past], test_y),\n",
    "epochs=25, batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 561,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13/13 [==============================] - 3s 114ms/step - loss: 0.8470\n",
      "test loss, test acc: 0.8469723463058472\n"
     ]
    }
   ],
   "source": [
    "results = model_lstm.evaluate([test_txt,test_aud,test_past], test_y, batch_size=8)\n",
    "print(\"test loss, test acc:\", results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 562,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "# from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.models import model_from_json\n",
    "\n",
    "# serialize model to JSON\n",
    "model_json = model_lstm.to_json()\n",
    "with open(\"model_lstm.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model_lstm.save_weights(\"model_lstm.h5\")\n",
    "print(\"Saved model to disk\")\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 563,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n",
      "mean_squared_error: 0.8470\n"
     ]
    }
   ],
   "source": [
    "# load json and create model\n",
    "json_file = open('model_lstm.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "loaded_model.load_weights(\"model_lstm.h5\")\n",
    "print(\"Loaded model from disk\")\n",
    " \n",
    "# evaluate loaded model on test data\n",
    "loaded_model.compile(loss='mean_squared_error', optimizer=opt,metrics=['mean_squared_error'])\n",
    "score = loaded_model.evaluate([test_txt,test_aud,test_past],test_y, verbose=0)\n",
    "print(\"%s: %.4f\" % (loaded_model.metrics_names[1], score[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 518,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tensorflow.python.keras.engine.functional.Functional object at 0x1e824b460>\n",
      "Model: \"model_20\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_111 (InputLayer)          [(None, 444, 768)]   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_112 (InputLayer)          [(None, 444, 29)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_118 (Bidirectiona (None, 444, 256)     918528      input_111[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_119 (Bidirectiona (None, 444, 128)     48128       input_112[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_175 (Dense)               (None, 444, 1)       257         bidirectional_118[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "dense_176 (Dense)               (None, 444, 1)       129         bidirectional_119[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_86 (Concatenate)    (None, 444, 2)       0           dense_175[0][0]                  \n",
      "                                                                 dense_176[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lstm_124 (LSTM)                 (None, 444, 1)       16          concatenate_86[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "reshape_40 (Reshape)            (None, 444)          0           lstm_124[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_177 (Dense)               (None, 128)          56960       reshape_40[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_178 (Dense)               (None, 64)           8256        dense_177[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_179 (Dense)               (None, 1)            65          dense_178[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "input_113 (InputLayer)          [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_87 (Concatenate)    (None, 2)            0           dense_179[0][0]                  \n",
      "                                                                 input_113[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_180 (Dense)               (None, 1)            3           concatenate_87[0][0]             \n",
      "==================================================================================================\n",
      "Total params: 1,032,342\n",
      "Trainable params: 1,032,342\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(loaded_model) \n",
    "print(loaded_model.summary())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
